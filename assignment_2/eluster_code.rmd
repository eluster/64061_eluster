---
title: "Assignment 2"
author: "Ethan Luster"
date: "11/7/2021"
output:
  pdf_document: default
  html_document: default
  word_document: default
---


```{r, warning=FALSE}
# The following five lines of code set-up the correct environment in RStudio
library(reticulate)
use_condaenv('r-reticulate')
library(tensorflow)
library(keras)
tf$constant("Hellow")

original_dataset_dir <- "C:/Users/13303/Downloads/kaggle_original_data" # Sets path for initial data set
base_dir <- "C:/Users/13303/Downloads/cats_and_dogs_small" # creates new directory

# The following code creates the necessary file paths for training, validation, and testing
dir.create(base_dir)
train_dir <- file.path(base_dir, "train")
dir.create(train_dir)
validation_dir <- file.path(base_dir, "validation")
dir.create(validation_dir)
test_dir <- file.path(base_dir, "test")
dir.create(test_dir)
train_cats_dir <- file.path(train_dir, "cats")
dir.create(train_cats_dir)
train_dogs_dir <- file.path(train_dir, "dogs")
dir.create(train_dogs_dir)
validation_cats_dir <- file.path(validation_dir, "cats")
dir.create(validation_cats_dir)
validation_dogs_dir <- file.path(validation_dir, "dogs")
dir.create(validation_dogs_dir)
test_cats_dir <- file.path(test_dir, "cats")
dir.create(test_cats_dir)
test_dogs_dir <- file.path(test_dir, "dogs")
dir.create(test_dogs_dir)

# The following code moves images to the correct data set
fnames <- paste0("cat.", 1:1000, ".jpg")
file.copy(file.path(original_dataset_dir, fnames), 
          file.path(train_cats_dir)) 
fnames <- paste0("cat.", 1001:1500, ".jpg")
file.copy(file.path(original_dataset_dir, fnames), 
          file.path(validation_cats_dir))
fnames <- paste0("cat.", 1501:2000, ".jpg")
file.copy(file.path(original_dataset_dir, fnames),
          file.path(test_cats_dir))
fnames <- paste0("dog.", 1:1000, ".jpg")
file.copy(file.path(original_dataset_dir, fnames),
          file.path(train_dogs_dir))
fnames <- paste0("dog.", 1001:1500, ".jpg")
file.copy(file.path(original_dataset_dir, fnames),
          file.path(validation_dogs_dir)) 
fnames <- paste0("dog.", 1501:2000, ".jpg")
file.copy(file.path(original_dataset_dir, fnames),
          file.path(test_dogs_dir))


```

The next section of code shows the most basic model for predicting whether an image is cats or dogs.
It is not based on any of the questions, but rather to show the base the questions are answered on.

```{r, warning=FALSE}
# Builds the base model
model <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = "relu", input_shape = c(150,150,3)) %>%
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3,3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3,3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  layer_flatten() %>%
  layer_dense(units=512, activation = "relu") %>%
  layer_dense(units=1, activation = "sigmoid") 

summary(model) # gives a summary of first model for reference

# compiles the model
model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(lr = 1e-4),
  metrics = c("acc")
)

train_datagen <- image_data_generator(rescale = 1.0/255) # rescales the training pictures
validation_datagen <- image_data_generator(rescale = 1.0/255) # rescales the validation pictures

# training generator
train_generator <- flow_images_from_directory(
  train_dir,
  train_datagen,
  target_size = c(150,150),
  batch_size = 20,
  class_mode = "binary"
)
# validation generator
validation_generator <- flow_images_from_directory(
  validation_dir,
  validation_datagen,
  target_size = c(150, 150),
  batch_size = 20,
  class_mode = "binary"
)
batch <- generator_next(train_generator)
str(batch)
# Runs the model
history <- model %>% fit_generator(
  train_generator,
  steps_per_epoch = 100,
  epochs = 30,
  validation_data = validation_generator,
  validation_steps = 50
)
plot(history) # plots the first model
```
The first model is highly overfitted. Thus, we will seek to increase the accuracy and
decrease the overfitting in future models.


# Question 1: 

The following code improves upon the base model by adding a dropout layer. This should
help the model to stop overfitting.

```{r, warning=FALSE}
# To reduce overfitting, imagedatagenerator is used to transform the images
datagen <- image_data_generator(
  rescale = 1/255,
  rotation_range = 40,
  width_shift_range = 0.2,
  height_shift_range = 0.2,
  shear_range = 0.2,
  zoom_range = 0.2,
  horizontal_flip = TRUE
)

# Randomly augments the images and displays an example
fnames <- list.files(file.path(train_dir,"cats"), full.names = T)
img_path <- fnames[[round(runif(1,1,length(fnames)))]]
img <- image_load(img_path, target_size = c(150,150))
img_array <- image_to_array(img)
img_array <- array_reshape(img_array, c(1,150,150,3))

augmentation_generator <- flow_images_from_data(
	img_array,
	generator = datagen,
	batch_size = 1
)

op <- par(mfrow=c(2,2), pty="s", mar=c(1,0,.1,0))
for (i in 1:4) {
  batch <- generator_next(augmentation_generator)
  plot(as.raster(batch[1,,,]))
}
par(op)
# Builds the second model for question 1
model2 <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = "relu", input_shape = c(150,150,3)) %>%
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3,3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3,3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  layer_flatten() %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units=512, activation = "relu") %>%
  layer_dense(units=1, activation = "sigmoid") 
# Compiles model
model2 %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(lr = 1e-4),
  metrics = c("acc")
)


test_datagen <- image_data_generator(rescale = 1/255) # transforms the testing images
# Builds training
train_generator <- flow_images_from_directory(
  train_dir,
  datagen,
  target_size = c(150, 150),
  batch_size = 20,
  class_mode = "binary"
)
# Builds validation
validation_generator <- flow_images_from_directory(
  validation_dir,
  test_datagen,
  target_size = c(150, 150),
  batch_size = 20,
  class_mode = "binary"
)
# Runs the model
history2 <- model2 %>% fit_generator(
  train_generator,
  steps_per_epoch = 100,
  epochs = 50,
  validation_data = validation_generator,
  validation_steps = 50
)

test_datagen <- image_data_generator(rescale = 1/255)
# This is the testing generator needed to test the model
test_generator <- flow_images_from_directory(
  test_dir,
  test_datagen,
  target_size = c(150, 150),
  batch_size = 20,
  class_mode = "binary"
)
plot(history2)

# Tests the model for loss and accuracy
model2 %>% evaluate_generator(test_generator, steps = 50)


```
After 50 epochs, the loss is .53, and the accuracy is .75

# Question 2
Next, we will add more pictures to the training data, while keeping the number of 
validation and test images the same
```{r, warning=FALSE}
dir.create(base_dir)
train_dir <- file.path(base_dir, "train")
dir.create(train_dir)
validation_dir <- file.path(base_dir, "validation")
dir.create(validation_dir)
test_dir <- file.path(base_dir, "test")
dir.create(test_dir)
train_cats_dir <- file.path(train_dir, "cats")
dir.create(train_cats_dir)
train_dogs_dir <- file.path(train_dir, "dogs")
dir.create(train_dogs_dir)
validation_cats_dir <- file.path(validation_dir, "cats")
dir.create(validation_cats_dir)
validation_dogs_dir <- file.path(validation_dir, "dogs")
dir.create(validation_dogs_dir)
test_cats_dir <- file.path(test_dir, "cats")
dir.create(test_cats_dir)
test_dogs_dir <- file.path(test_dir, "dogs")
dir.create(test_dogs_dir)

# The following code changes the number of images to increase the training set
fnames <- paste0("cat.", 1:1500, ".jpg")
file.copy(file.path(original_dataset_dir, fnames), 
          file.path(train_cats_dir)) 
fnames <- paste0("cat.", 1501:2000, ".jpg")
file.copy(file.path(original_dataset_dir, fnames), 
          file.path(validation_cats_dir))
fnames <- paste0("cat.", 2001:2500, ".jpg")
file.copy(file.path(original_dataset_dir, fnames),
          file.path(test_cats_dir))
fnames <- paste0("dog.", 1:1500, ".jpg")
file.copy(file.path(original_dataset_dir, fnames),
          file.path(train_dogs_dir))
fnames <- paste0("dog.", 1501:2000, ".jpg")
file.copy(file.path(original_dataset_dir, fnames),
          file.path(validation_dogs_dir)) 
fnames <- paste0("dog.", 2001:2500, ".jpg")
file.copy(file.path(original_dataset_dir, fnames),
          file.path(test_dogs_dir))


datagen <- image_data_generator(
  rescale = 1/255,
  rotation_range = 40,
  width_shift_range = 0.2,
  height_shift_range = 0.2,
  shear_range = 0.2,
  zoom_range = 0.2,
  horizontal_flip = TRUE
)


fnames <- list.files(file.path(train_dir,"cats"), full.names = T)
img_path <- fnames[[round(runif(1,1,length(fnames)))]]
img <- image_load(img_path, target_size = c(150,150))
img_array <- image_to_array(img)
img_array <- array_reshape(img_array, c(1,150,150,3))

augmentation_generator <- flow_images_from_data(
	img_array,
	generator = datagen,
	batch_size = 1
)

op <- par(mfrow=c(2,2), pty="s", mar=c(1,0,.1,0))
for (i in 1:4) {
  batch <- generator_next(augmentation_generator)
  plot(as.raster(batch[1,,,]))
}
par(op)

model3 <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = "relu", input_shape = c(150,150,3)) %>%
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3,3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3,3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  layer_flatten() %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units=512, activation = "relu") %>%
  layer_dense(units=1, activation = "sigmoid") 

model3 %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(lr = 1e-4),
  metrics = c("acc")
)


test_datagen <- image_data_generator(rescale = 1/255)

train_generator <- flow_images_from_directory(
  train_dir,
  datagen,
  target_size = c(150, 150),
  batch_size = 25,
  class_mode = "binary"
)
validation_generator <- flow_images_from_directory(
  validation_dir,
  test_datagen,
  target_size = c(150, 150),
  batch_size = 25,
  class_mode = "binary"
)
history3 <- model3 %>% fit_generator(
  train_generator,
  steps_per_epoch = 100,
  epochs = 50,
  validation_data = validation_generator,
  validation_steps = 50
)


test_generator <- flow_images_from_directory(
  test_dir,
  test_datagen,
  target_size = c(150, 150),
  batch_size = 20,
  class_mode = "binary"
)
plot(history3)
model3 %>% evaluate_generator(test_generator, steps = 50)
```
The model has a loss of .49 and an accuracy of .78 . This is a slight improvement of
about 3 percent over the model with less training images.


# Question 3
The following model has the same number of training images, but increases the number
of validation and testing images by 500 each.
```{r, warning=FALSE}
dir.create(base_dir)
train_dir <- file.path(base_dir, "train")
dir.create(train_dir)
validation_dir <- file.path(base_dir, "validation")
dir.create(validation_dir)
test_dir <- file.path(base_dir, "test")
dir.create(test_dir)
train_cats_dir <- file.path(train_dir, "cats")
dir.create(train_cats_dir)
train_dogs_dir <- file.path(train_dir, "dogs")
dir.create(train_dogs_dir)
validation_cats_dir <- file.path(validation_dir, "cats")
dir.create(validation_cats_dir)
validation_dogs_dir <- file.path(validation_dir, "dogs")
dir.create(validation_dogs_dir)
test_cats_dir <- file.path(test_dir, "cats")
dir.create(test_cats_dir)
test_dogs_dir <- file.path(test_dir, "dogs")
dir.create(test_dogs_dir)

# The following code changes the image numbers again to also increase validation and testing datasets
fnames <- paste0("cat.", 1:1500, ".jpg")
file.copy(file.path(original_dataset_dir, fnames), 
          file.path(train_cats_dir)) 
fnames <- paste0("cat.", 1501:2250, ".jpg")
file.copy(file.path(original_dataset_dir, fnames), 
          file.path(validation_cats_dir))
fnames <- paste0("cat.", 2251:3000, ".jpg")
file.copy(file.path(original_dataset_dir, fnames),
          file.path(test_cats_dir))
fnames <- paste0("dog.", 1:1500, ".jpg")
file.copy(file.path(original_dataset_dir, fnames),
          file.path(train_dogs_dir))
fnames <- paste0("dog.", 1501:2250, ".jpg")
file.copy(file.path(original_dataset_dir, fnames),
          file.path(validation_dogs_dir)) 
fnames <- paste0("dog.", 2251:3000, ".jpg")
file.copy(file.path(original_dataset_dir, fnames),
          file.path(test_dogs_dir))


datagen <- image_data_generator(
  rescale = 1/255,
  rotation_range = 40,
  width_shift_range = 0.2,
  height_shift_range = 0.2,
  shear_range = 0.2,
  zoom_range = 0.2,
  horizontal_flip = TRUE
)


fnames <- list.files(file.path(train_dir,"cats"), full.names = T)
img_path <- fnames[[round(runif(1,1,length(fnames)))]]
img <- image_load(img_path, target_size = c(150,150))
img_array <- image_to_array(img)
img_array <- array_reshape(img_array, c(1,150,150,3))

augmentation_generator <- flow_images_from_data(
	img_array,
	generator = datagen,
	batch_size = 1
)

op <- par(mfrow=c(2,2), pty="s", mar=c(1,0,.1,0))
for (i in 1:4) {
  batch <- generator_next(augmentation_generator)
  plot(as.raster(batch[1,,,]))
}
par(op)

model4 <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = "relu", input_shape = c(150,150,3)) %>%
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3,3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3,3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  layer_flatten() %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units=512, activation = "relu") %>%
  layer_dense(units=1, activation = "sigmoid") 

model4 %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(lr = 1e-4),
  metrics = c("acc")
)


test_datagen <- image_data_generator(rescale = 1/255)

train_generator <- flow_images_from_directory(
  train_dir,
  datagen,
  target_size = c(150, 150),
  batch_size = 20,
  class_mode = "binary"
)
validation_generator <- flow_images_from_directory(
  validation_dir,
  test_datagen,
  target_size = c(150, 150),
  batch_size = 20,
  class_mode = "binary"
)
history4 <- model4 %>% fit_generator(
  train_generator,
  steps_per_epoch = 100,
  epochs = 50,
  validation_data = validation_generator,
  validation_steps = 50
)

test_datagen <- image_data_generator(rescale = 1/255)
test_generator <- flow_images_from_directory(
  test_dir,
  test_datagen,
  target_size = c(150, 150),
  batch_size = 20,
  class_mode = "binary"
)
plot(history4)
model4 %>% evaluate_generator(test_generator, steps = 50)

```
The model has a loss of .51 and an accuarcy of .763. This is slightly worse than the model
with less training and validation data.

# Question 4:
The final model uses the pretrained network by using feature extraction
```{r, warning=FALSE}
# Creates a new convolutional base  
conv_base <- application_vgg16(
    weights = "imagenet",
    include_top = FALSE,
    input_shape = c(150, 150, 3)
  )

  base_dir <- "C:/Users/13303/Downloads/cats_and_dogs_small"
  train_dir <- train_dir <- file.path(base_dir, "train")
  validation_dir <- validation_dir <- file.path(base_dir, "validation")
  test_dir <- test_dir <- file.path(base_dir, "test")  
  datagen <- image_data_generator(rescale = 1/255)
  batch_size <- 20
  extract_features <- function(directory, sample_count) {
    features <- array(0, dim = c(sample_count, 4, 4, 512))
    labels <- array(0, dim = c(sample_count))
    generator <- flow_images_from_directory(
      directory = directory,
      generator = datagen,
      target_size = c(150, 150),
      batch_size = batch_size,
      class_mode = "binary"
    )
    i <- 0
    while(TRUE) {
      batch <- generator_next(generator)
      inputs_batch <- batch[[1]]
      labels_batch <- batch[[2]]
      features_batch <- conv_base %>% predict(inputs_batch)
      index_range <- ((i * batch_size)+1):((i+1) * batch_size)
      features[index_range,,,] <- features_batch
      labels[index_range] <- labels_batch
      i <- i + 1
      if (i * batch_size >= sample_count)
        break
    }
    list(
      features = features,
      labels = labels
    )
  }  
  train <- extract_features(train_dir, 3000) # extracts features from training
  validation <- extract_features(validation_dir, 1500) # extracts from validation
  test <- extract_features(test_dir, 1500) # extracts from training
  
  reshape_features <- function(features) {
    array_reshape(features, dim = c(nrow(features), 4 * 4 * 512))  
  }
  train$features <-reshape_features(train$features)
  validation$features <- reshape_features(validation$features)
  test$features <- reshape_features(test$features)  
  
  model5 <- keras_model_sequential() %>%
    layer_dense(units = 256, activation = "relu", input_shape = 4 * 4 * 512) %>%
    layer_dropout(rate = 0.5) %>%
    layer_dense(units = 1, activation = "sigmoid")
  model5 %>% compile(
    optimizer = optimizer_rmsprop(lr = 2e-5),
    loss = "binary_crossentropy",
    metrics = c("accuracy")
  )  
  History5 <- model5 %>% fit(
    train$features, train$labels,
    epochs = 30,
    batch_size = 20,
    validation_data = list(validation$features, validation$labels)
  )
  
  plot(History5)

  model5 %>% evaluate(test$features, test$labels)

```
Model 5 has a loss of .24 and and an accuracy of .896. Based on the graphs, it shows a higher
level of overfitting than the previous models, but less overfitting than the first model.




  

  
  
  
  
  